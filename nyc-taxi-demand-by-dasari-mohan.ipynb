{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Taxi demand prediction in New York City\n\n                                                                            Code Written By : Dasari Mohana\n### Context\nThe yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP).                                                                           \n\n## Data Information\n<p>\nGe the data from : http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml (2016 data)\nThe data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) \n</p>\n\n## Information on taxis:\n\n<h5> Yellow Taxi: Yellow Medallion Taxicabs</h5>\n<p> These are the famous NYC yellow taxis that provide transportation exclusively through street-hails. The number of taxicabs is limited by a finite number of medallions issued by the TLC. You access this mode of transportation by standing in the street and hailing an available taxi with your hand. The pickups are not pre-arranged.</p>\n\n<h5> For Hire Vehicles (FHVs) </h5>\n<p> FHV transportation is accessed by a pre-arrangement with a dispatcher or limo company. These FHVs are not permitted to pick up passengers via street hails, as those rides are not considered pre-arranged. </p>\n\n<h5> Green Taxi: Street Hail Livery (SHL) </h5>\n<p>  The SHL program will allow livery vehicle owners to license and outfit their vehicles with green borough taxi branding, meters, credit card machines, and ultimately the right to accept street hails in addition to pre-arranged rides. </p>\n<p> Credits: Quora</p>\n\n<h5>Footnote:</h5>\nIn the given notebook we are considering only the yellow taxis for the time period between Jan - Mar 2015 & Jan - Mar 2016","metadata":{"id":"WROuP70HpYRh"}},{"cell_type":"code","source":"#Importing Libraries\n!pip3 install graphviz\n!pip3 install dask\n!pip install \"dask[complete]\" \n!pip3 install toolz\n!pip3 install cloudpickle\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np \n\n!pip3 install folium\nimport folium #open street map\n\nimport datetime # Convert to unix time\nimport time #Convert to unix time\n\nimport matplotlib\n\n# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user interactive like zoom in and zoom out\nmatplotlib.use('nbagg')\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\n\n# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n!pip install gpxpy\nimport gpxpy.geo #Get the haversine distance\n\n#Clustering\nfrom sklearn.cluster import MiniBatchKMeans, KMeans \nimport math\nimport pickle\nimport os\n\n# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"executionInfo":{"elapsed":27569,"status":"ok","timestamp":1631255182223,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"KwvQLV8ypYRk","outputId":"dcaf97a2-94c5-46bd-96d0-484a04ec9cb7","execution":{"iopub.status.busy":"2021-12-01T07:48:14.658671Z","iopub.execute_input":"2021-12-01T07:48:14.659084Z","iopub.status.idle":"2021-12-01T07:49:06.332679Z","shell.execute_reply.started":"2021-12-01T07:48:14.658981Z","shell.execute_reply":"2021-12-01T07:49:06.331714Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Collection\nWe Have collected all yellow taxi trips data from jan-2015 to dec-2016(Will be using only 2015 data)\n<table>\n<tr>\n<th> file name </th>\n<th> file name size</th>\n<th> number of records </th>\n<th> number of features </th>\n</tr>\n<tr>\n<td> yellow_tripdata_2016-01 </td>\n<td> 1. 59G </td>\n<td> 10906858 </td>\n<td> 19 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-02 </td>\n<td> 1. 66G </td>\n<td> 11382049 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2016-03 </td>\n<td> 1. 78G </td>\n<td> 12210952 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2016-04 </td>\n<td> 1. 74G </td>\n<td> 11934338 </td>\n<td> 19 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-05 </td>\n<td> 1. 73G </td>\n<td> 11836853 </td>\n<td> 19 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-06 </td>\n<td> 1. 62G </td>\n<td> 11135470 </td>\n<td> 19 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-07 </td>\n<td> 884Mb </td>\n<td> 10294080 </td>\n<td> 17 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-08 </td>\n<td> 854Mb </td>\n<td> 9942263 </td>\n<td> 17 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-09 </td>\n<td> 870Mb </td>\n<td> 10116018 </td>\n<td> 17 </td>\n</tr>\n\n<tr>\n<td> yellow_tripdata_2016-10 </td>\n<td> 933Mb </td>\n<td> 10854626 </td>\n<td> 17 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2016-11 </td>\n<td> 868Mb </td>\n<td> 10102128 </td>\n<td> 17 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2016-12 </td>\n<td> 897Mb </td>\n<td> 10449408 </td>\n<td> 17 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-01 </td>\n<td> 1.84Gb </td>\n<td> 12748986 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-02 </td>\n<td> 1.81Gb </td>\n<td> 12450521 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-03 </td>\n<td> 1.94Gb </td>\n<td> 13351609 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-04 </td>\n<td> 1.90Gb </td>\n<td> 13071789 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-05 </td>\n<td> 1.91Gb </td>\n<td> 13158262 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-06 </td>\n<td> 1.79Gb </td>\n<td> 12324935 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-07 </td>\n<td> 1.68Gb </td>\n<td> 11562783 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-08 </td>\n<td> 1.62Gb </td>\n<td> 11130304 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-09 </td>\n<td> 1.63Gb </td>\n<td> 11225063 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-10 </td>\n<td> 1.79Gb </td>\n<td> 12315488 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-11 </td>\n<td> 1.65Gb </td>\n<td> 11312676 </td>\n<td> 19 </td>\n</tr>\n<tr>\n<td> yellow_tripdata_2015-12 </td>\n<td> 1.67Gb </td>\n<td> 11460573 </td>\n<td> 19 </td>\n</tr>\n</table>","metadata":{"id":"nIrfeCeZpYRq"}},{"cell_type":"markdown","source":"# ML Problem Formulation\n<p><b> Time-series forecasting and Regression</b></p>\n<br>\n- To find number of pickups, given location cordinates(latitude and longitude) and time, in the query reigion and surrounding regions.</i>\n<p> \n- To solve the above we would be using data collected in Jan - Mar 2015 to predict the pickups in Jan - Mar 2016.\n</p>","metadata":{"id":"vKwBOhUWpYR5"}},{"cell_type":"markdown","source":"# Performance metrics\n\n1. Mean Absolute percentage error.\n2. Mean Squared error.","metadata":{"id":"E-FREjPopYR7"}},{"cell_type":"markdown","source":"## Data Cleaning\n\nIn this section I will be doing univariate analysis and removing outlier/illegitimate values which may be caused due to some human or data storage error","metadata":{"id":"WGj_7pHlpYR8"}},{"cell_type":"code","source":"# table below shows few datapoints along with all our features\nmonth = dd.read_csv('../input/newyork-taxi-demand/yellow_tripdata_2015-01.csv')\nprint(month.columns)\nmonth.head(5)","metadata":{"executionInfo":{"elapsed":1861,"status":"ok","timestamp":1631255328244,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"bG0OZavUpYR9","outputId":"2727fb04-60aa-4182-99c8-0dad9e823ebb","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T07:49:06.334420Z","iopub.execute_input":"2021-12-01T07:49:06.334669Z","iopub.status.idle":"2021-12-01T07:49:08.848077Z","shell.execute_reply.started":"2021-12-01T07:49:06.334634Z","shell.execute_reply":"2021-12-01T07:49:08.847149Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Visualization\nmonth.visualize()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:49:08.849776Z","iopub.execute_input":"2021-12-01T07:49:08.850369Z","iopub.status.idle":"2021-12-01T07:49:09.797879Z","shell.execute_reply.started":"2021-12-01T07:49:08.850324Z","shell.execute_reply":"2021-12-01T07:49:09.796960Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Visualizing fare amount\nmonth.fare_amount.sum().visualize()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:49:09.799210Z","iopub.execute_input":"2021-12-01T07:49:09.799457Z","iopub.status.idle":"2021-12-01T07:49:10.732807Z","shell.execute_reply.started":"2021-12-01T07:49:09.799426Z","shell.execute_reply":"2021-12-01T07:49:10.731863Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 1. Pickup Latitude and Pickup Longitude","metadata":{"id":"YpwPiJzipYSB"}},{"cell_type":"markdown","source":"It is inferred from the source https://www.flickr.com/places/info/2459115 that New York is bounded by the location cordinates(lat,long) - (40.5774, -74.15) & (40.9176,-73.7004) so hence any cordinates not within these cordinates are not considered by us as we are only concerned with pickups which originate within New York.","metadata":{"id":"RocDjcsdpYSC"}},{"cell_type":"code","source":"# Plotting pickup cordinates which are outside the bounding box of New-York \n# collect all the points outside the bounding box of newyork city to outlier_locations\n\noutlier_locations = month[((month.pickup_longitude <= -74.15) | (month.pickup_latitude <= 40.5774)| \\\n                   (month.pickup_longitude >= -73.7004) | (month.pickup_latitude >= 40.9176))]\n\n# creating a map with the a base location\nmap_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n\n# Here I spot only first 100 outliers on the map, plotting all the outliers will take more time\nsample_locations = outlier_locations.head(10000)\nfor i,j in sample_locations.iterrows():\n    if int(j['pickup_latitude']) != 0:\n        folium.Marker(list((j['pickup_latitude'],j['pickup_longitude']))).add_to(map_osm)\nmap_osm","metadata":{"executionInfo":{"elapsed":4404,"status":"ok","timestamp":1615629815914,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"HWLsXmMfpYSD","outputId":"b5141e6a-b203-44c7-af8c-974c3e02fa1d","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T07:49:10.735409Z","iopub.execute_input":"2021-12-01T07:49:10.735756Z","iopub.status.idle":"2021-12-01T07:49:12.486429Z","shell.execute_reply.started":"2021-12-01T07:49:10.735703Z","shell.execute_reply":"2021-12-01T07:49:12.485446Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<b>Observation:\n</b> We can see in above graph that there are some points just outside the boundary of Newyork city (Newyork State and other states) but there are a few that are in either South america, Mexico and even in Ocean.","metadata":{"id":"znO3Z9ocpYSH"}},{"cell_type":"markdown","source":"### 2. Dropoff Latitude & Dropoff Longitude","metadata":{"id":"BI8HYkIOpYSH"}},{"cell_type":"markdown","source":"It is inferred from the source https://www.flickr.com/places/info/2459115 that New York is bounded by the location cordinates(lat,long) - (40.5774, -74.15) & (40.9176,-73.7004) so hence any cordinates not within these cordinates are not considered by us as we are only concerned with dropoffs which are within New York.","metadata":{"id":"_W_1MXhrpYSI"}},{"cell_type":"code","source":"# Plotting dropoff cordinates which are outside the bounding box of New-York \n# we will collect all the points outside the bounding box of newyork city to outlier_locations\noutlier_locations = month[((month.dropoff_longitude <= -74.15) | (month.dropoff_latitude <= 40.5774)| \\\n                   (month.dropoff_longitude >= -73.7004) | (month.dropoff_latitude >= 40.9176))]\n\n# creating a map with the a base location\nmap_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n\n# we will spot only first 100 outliers on the map, plotting all the outliers will take more time\nsample_locations = outlier_locations.head(10000)\nfor i,j in sample_locations.iterrows():\n    if int(j['pickup_latitude']) != 0:\n        folium.Marker(list((j['dropoff_latitude'],j['dropoff_longitude']))).add_to(map_osm)\nmap_osm","metadata":{"executionInfo":{"elapsed":4417,"status":"ok","timestamp":1615629828907,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"52GGm00KpYSJ","outputId":"616dcdd0-3567-4a6d-bc35-b58905162e8d","execution":{"iopub.status.busy":"2021-12-01T07:49:12.487829Z","iopub.execute_input":"2021-12-01T07:49:12.488145Z","iopub.status.idle":"2021-12-01T07:49:15.017519Z","shell.execute_reply.started":"2021-12-01T07:49:12.488106Z","shell.execute_reply":"2021-12-01T07:49:15.016637Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<b>Observation:\n</b> The observations here are similar to those obtained while analyzing pickup latitude and longitude","metadata":{"id":"pz-fvF1MpYSN"}},{"cell_type":"markdown","source":"### 3. Trip Durations:","metadata":{"id":"ZSmVwmMJpYSO"}},{"cell_type":"markdown","source":"<p style=\"font-size:18px\">According to NYC Taxi &amp; Limousine Commision Regulations : <b style= \"color:red\">The maximum allowed trip duration in a 24 hour interval is only 12 hours (720 mins).</b> </p>","metadata":{"id":"DuilRJY1pYSP"}},{"cell_type":"code","source":"# The timestamps are converted to unix so as to get duration(trip-time) & speed also pickup-times in unix are used while binning \n\n# in out data we have time in the formate \"YYYY-MM-DD HH:MM:SS\" we convert this sting to python time formate and then into unix time stamp\n\ndef convert_to_unix(s):\n    return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())\n\ndef return_with_trip_times(month):\n    \n    duration = month[['tpep_pickup_datetime','tpep_dropoff_datetime']].compute()\n    \n    # pickups and dropoffs to unix time\n    duration_pickup = [convert_to_unix(x) for x in duration['tpep_pickup_datetime'].values]\n    duration_drop = [convert_to_unix(x) for x in duration['tpep_dropoff_datetime'].values]\n    \n    #calculate duration of trips\n    durations = (np.array(duration_drop) - np.array(duration_pickup))/float(60)\n\n    # append durations of trips and speed in miles/hr to a new dataframe\n    new_frame = month[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']].compute()\n    \n    new_frame['trip_times'] = durations\n    new_frame['pickup_times'] = duration_pickup\n    new_frame['Speed'] = 60*(new_frame['trip_distance']/new_frame['trip_times'])\n    \n    return new_frame\n\nframe_with_durations = return_with_trip_times(month)\nprint(frame_with_durations.head())","metadata":{"id":"1Qa9aMyFpYSQ","execution":{"iopub.status.busy":"2021-12-01T07:49:15.018876Z","iopub.execute_input":"2021-12-01T07:49:15.019119Z","iopub.status.idle":"2021-12-01T07:57:16.891463Z","shell.execute_reply.started":"2021-12-01T07:49:15.019090Z","shell.execute_reply":"2021-12-01T07:57:16.890464Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# the skewed box plot shows us the presence of outliers \n%matplotlib inline\nsns.boxplot(y=\"trip_times\", data = frame_with_durations)\nplt.show()","metadata":{"executionInfo":{"elapsed":456137,"status":"ok","timestamp":1615630308946,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"IZ8eoLpXpYSS","outputId":"61ac9324-be74-4578-f1f1-5941e2f4f632","execution":{"iopub.status.busy":"2021-12-01T07:57:16.893028Z","iopub.execute_input":"2021-12-01T07:57:16.893407Z","iopub.status.idle":"2021-12-01T07:57:18.795056Z","shell.execute_reply.started":"2021-12-01T07:57:16.893345Z","shell.execute_reply":"2021-12-01T07:57:18.794122Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# calculating 0-100th percentile to find a the correct percentile value for removal of outliers\n\nfor i in range(0,100,10):\n    var =frame_with_durations[\"trip_times\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"executionInfo":{"elapsed":464488,"status":"ok","timestamp":1615630318620,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"ctVQ6FTWpYSX","outputId":"af3a035d-ed34-47f0-8b5d-6ad81c75c879","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T07:57:18.796207Z","iopub.execute_input":"2021-12-01T07:57:18.796452Z","iopub.status.idle":"2021-12-01T07:57:28.884955Z","shell.execute_reply.started":"2021-12-01T07:57:18.796424Z","shell.execute_reply":"2021-12-01T07:57:28.883727Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Looking further from the 99th percecntile\n\nfor i in range(90,100):\n    var =frame_with_durations[\"trip_times\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"executionInfo":{"elapsed":473794,"status":"ok","timestamp":1615630328902,"user":{"displayName":"Applied AI Course","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjpR94Niu_srJxsFT_3bp9vDfqrjd2Zx94jDQdMg=s64","userId":"06629147635963609455"},"user_tz":-330},"id":"8lGrGf0ZpYSd","outputId":"b1e43e23-22d2-403e-ce69-8d383ca9f32b","execution":{"iopub.status.busy":"2021-12-01T07:57:28.886232Z","iopub.execute_input":"2021-12-01T07:57:28.886571Z","iopub.status.idle":"2021-12-01T07:57:39.005263Z","shell.execute_reply.started":"2021-12-01T07:57:28.886529Z","shell.execute_reply":"2021-12-01T07:57:39.004153Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Removing data based on our analysis and TLC regulations\n\nframe_with_durations_modified=frame_with_durations[(frame_with_durations.trip_times>1) & \n                                                   (frame_with_durations.trip_times<720)]","metadata":{"id":"_ZHF1neQpYSh","execution":{"iopub.status.busy":"2021-12-01T07:57:39.006655Z","iopub.execute_input":"2021-12-01T07:57:39.006863Z","iopub.status.idle":"2021-12-01T07:57:40.389224Z","shell.execute_reply.started":"2021-12-01T07:57:39.006838Z","shell.execute_reply":"2021-12-01T07:57:40.388318Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Box-plot after removal of outliers\n\nsns.boxplot(y=\"trip_times\", data =frame_with_durations_modified)\nplt.show()","metadata":{"id":"0_o8FgO8pYSj","execution":{"iopub.status.busy":"2021-12-01T07:57:40.390421Z","iopub.execute_input":"2021-12-01T07:57:40.391049Z","iopub.status.idle":"2021-12-01T07:57:42.404063Z","shell.execute_reply.started":"2021-12-01T07:57:40.391013Z","shell.execute_reply":"2021-12-01T07:57:42.403217Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# converting the values to log-values to chec for log-normal\n\nimport math\nframe_with_durations_modified['log_times']=[math.log(i) for i in frame_with_durations_modified['trip_times'].values]","metadata":{"id":"o3hE6TkCpYSr","execution":{"iopub.status.busy":"2021-12-01T07:57:42.405194Z","iopub.execute_input":"2021-12-01T07:57:42.405412Z","iopub.status.idle":"2021-12-01T07:57:48.039115Z","shell.execute_reply.started":"2021-12-01T07:57:42.405386Z","shell.execute_reply":"2021-12-01T07:57:48.038134Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Q-Q plot for checking if trip-times is log-normal\nimport scipy\nscipy.stats.probplot(frame_with_durations_modified['log_times'].values, plot=plt)\nplt.show()","metadata":{"id":"dgLxnEzJpYSy","outputId":"65cab74f-fb64-4e38-a86a-891c6246ba9d","execution":{"iopub.status.busy":"2021-12-01T07:57:48.042010Z","iopub.execute_input":"2021-12-01T07:57:48.042224Z","iopub.status.idle":"2021-12-01T07:58:13.928732Z","shell.execute_reply.started":"2021-12-01T07:57:48.042198Z","shell.execute_reply":"2021-12-01T07:58:13.927605Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 4. Speed","metadata":{"id":"du_5d8mppYS1"}},{"cell_type":"code","source":"# check for any outliers in the data after trip duration outliers removed\n# box-plot for speeds with outliers\n\nframe_with_durations_modified['Speed'] = 60*(frame_with_durations_modified['trip_distance']/frame_with_durations_modified['trip_times'])\nsns.boxplot(y=\"Speed\", data =frame_with_durations_modified)\nplt.show()","metadata":{"id":"cYm5q8VFpYS3","outputId":"12e1cbd5-9a2b-403a-f1fa-d6766610868b","execution":{"iopub.status.busy":"2021-12-01T07:58:13.930148Z","iopub.execute_input":"2021-12-01T07:58:13.930401Z","iopub.status.idle":"2021-12-01T07:58:16.252206Z","shell.execute_reply.started":"2021-12-01T07:58:13.930361Z","shell.execute_reply":"2021-12-01T07:58:16.251249Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Calculating speed values at each percntile 0,10,20,30,40,50,60,70,80,90,100 \n\nfor i in range(0,100,10):\n    var =frame_with_durations_modified[\"Speed\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"Y0yi1ABQpYS6","outputId":"a381e749-451c-45e4-9456-a17c80b4410d","execution":{"iopub.status.busy":"2021-12-01T07:58:16.253499Z","iopub.execute_input":"2021-12-01T07:58:16.253709Z","iopub.status.idle":"2021-12-01T07:58:29.306036Z","shell.execute_reply.started":"2021-12-01T07:58:16.253685Z","shell.execute_reply":"2021-12-01T07:58:29.305172Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#calculating speed values at each percntile 90,91,92,93,94,95,96,97,98,99,100\nfor i in range(90,100):\n    var =frame_with_durations_modified[\"Speed\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"_r5dllIcpYS-","outputId":"f635dc37-59da-45b6-bb1f-98c23bf5cfdb","execution":{"iopub.status.busy":"2021-12-01T07:58:29.309172Z","iopub.execute_input":"2021-12-01T07:58:29.309617Z","iopub.status.idle":"2021-12-01T07:58:42.342250Z","shell.execute_reply.started":"2021-12-01T07:58:29.309582Z","shell.execute_reply":"2021-12-01T07:58:42.341269Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Calculating speed values at each percntile 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\n\nfor i in np.arange(0.0, 1.0, 0.1):\n    var =frame_with_durations_modified[\"Speed\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"-DnlaavLpYTB","outputId":"862cf5e5-e48f-414e-b700-a9f2c86e64e4","execution":{"iopub.status.busy":"2021-12-01T07:58:42.343749Z","iopub.execute_input":"2021-12-01T07:58:42.344430Z","iopub.status.idle":"2021-12-01T07:58:55.430841Z","shell.execute_reply.started":"2021-12-01T07:58:42.344359Z","shell.execute_reply":"2021-12-01T07:58:55.429905Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Removing further outliers based on the 99.9th percentile value\n\nframe_with_durations_modified=frame_with_durations[(frame_with_durations.Speed>0) & (frame_with_durations.Speed<45.31)]","metadata":{"id":"4tOl7JX6pYTG","execution":{"iopub.status.busy":"2021-12-01T07:58:55.432476Z","iopub.execute_input":"2021-12-01T07:58:55.432858Z","iopub.status.idle":"2021-12-01T07:58:56.207242Z","shell.execute_reply.started":"2021-12-01T07:58:55.432813Z","shell.execute_reply":"2021-12-01T07:58:56.206450Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Avg.speed of cabs in New-York\n\nsum(frame_with_durations_modified[\"Speed\"]) / float(len(frame_with_durations_modified[\"Speed\"]))","metadata":{"id":"8mN_lVT0pYTJ","outputId":"2ed6f2af-0804-4e80-f70f-51be92fd53f4","execution":{"iopub.status.busy":"2021-12-01T07:58:56.208430Z","iopub.execute_input":"2021-12-01T07:58:56.208657Z","iopub.status.idle":"2021-12-01T07:58:57.186185Z","shell.execute_reply.started":"2021-12-01T07:58:56.208632Z","shell.execute_reply":"2021-12-01T07:58:57.185416Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"<b style='font-size:16px'>The Average speed in Newyork speed is 12.45 miles/hr, therefore a cab driver can travel <font color='red'> 2 miles per 10 mins on an average.</font> </b>","metadata":{"id":"NTIxRcNHpYTS"}},{"cell_type":"markdown","source":"### 4. Trip Distance","metadata":{"id":"wXpRUuKTpYTT"}},{"cell_type":"code","source":"# Lets try if there are any outliers in trip distance\n# box-plot showing outliers in trip-distance values\n\nsns.boxplot(y=\"trip_distance\", data = frame_with_durations_modified)\nplt.show()","metadata":{"id":"ZirabsfNpYTU","outputId":"eeff0c02-fa70-412a-b301-92d481e8a60e","execution":{"iopub.status.busy":"2021-12-01T07:58:57.187257Z","iopub.execute_input":"2021-12-01T07:58:57.187826Z","iopub.status.idle":"2021-12-01T07:59:00.321339Z","shell.execute_reply.started":"2021-12-01T07:58:57.187782Z","shell.execute_reply":"2021-12-01T07:59:00.320728Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Calculating trip distance values at each percntile in range 0 to 100\n\nfor i in range(0,100,10):\n    \n    var =frame_with_durations_modified[\"trip_distance\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\n    \nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"bCSakQ_TpYTX","outputId":"4861bd72-cfd5-4ec9-dff4-663b12720d48","execution":{"iopub.status.busy":"2021-12-01T07:59:00.322237Z","iopub.execute_input":"2021-12-01T07:59:00.322614Z","iopub.status.idle":"2021-12-01T07:59:09.219936Z","shell.execute_reply.started":"2021-12-01T07:59:00.322569Z","shell.execute_reply":"2021-12-01T07:59:09.219084Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Calculating trip distance values at each percentile 90 to 100\n\nfor i in range(90,100):\n    \n    var =frame_with_durations_modified[\"trip_distance\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\n    \nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"ELszXDehpYTa","outputId":"1bce9682-ec9f-4a29-b065-f3ba1d651ba1","execution":{"iopub.status.busy":"2021-12-01T07:59:09.221288Z","iopub.execute_input":"2021-12-01T07:59:09.221604Z","iopub.status.idle":"2021-12-01T07:59:18.106425Z","shell.execute_reply.started":"2021-12-01T07:59:09.221564Z","shell.execute_reply":"2021-12-01T07:59:18.105506Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Calculating trip distance values at each percntile 99.0,99.1.....99.9,100\n\nfor i in np.arange(0.0, 1.0, 0.1):\n    \n    var =frame_with_durations_modified[\"trip_distance\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)/100))]))\n    \nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"-qFqC3TDpYTc","outputId":"1f3a2d6e-4de7-4383-fd82-e8ee4abe32d9","execution":{"iopub.status.busy":"2021-12-01T07:59:18.107812Z","iopub.execute_input":"2021-12-01T07:59:18.108094Z","iopub.status.idle":"2021-12-01T07:59:27.014094Z","shell.execute_reply.started":"2021-12-01T07:59:18.108056Z","shell.execute_reply":"2021-12-01T07:59:27.013272Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Removing further outliers based on the 99.9th percentile value\n\nframe_with_durations_modified=frame_with_durations[(frame_with_durations.trip_distance>0) & (frame_with_durations.trip_distance<23)]","metadata":{"id":"zvbqa_oGpYTf","execution":{"iopub.status.busy":"2021-12-01T07:59:27.015251Z","iopub.execute_input":"2021-12-01T07:59:27.015482Z","iopub.status.idle":"2021-12-01T07:59:27.612254Z","shell.execute_reply.started":"2021-12-01T07:59:27.015455Z","shell.execute_reply":"2021-12-01T07:59:27.611454Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Box-plot after removal of outliers\nsns.boxplot(y=\"trip_distance\", data = frame_with_durations_modified)\nplt.show()","metadata":{"id":"x5IQIQjFpYTh","outputId":"b85a5f57-a333-42d9-a83e-f1e35cbdc1f6","execution":{"iopub.status.busy":"2021-12-01T07:59:27.613716Z","iopub.execute_input":"2021-12-01T07:59:27.614075Z","iopub.status.idle":"2021-12-01T07:59:30.718188Z","shell.execute_reply.started":"2021-12-01T07:59:27.614035Z","shell.execute_reply":"2021-12-01T07:59:30.717225Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### 5. Total Fare","metadata":{"id":"cXWwikpxpYTl"}},{"cell_type":"code","source":"# box-plot showing outliers in fare\n\nsns.boxplot(y=\"total_amount\", data =frame_with_durations_modified)\nplt.show()","metadata":{"id":"oYrsaxbBpYTm","outputId":"e9d17a41-faea-41f6-b2ca-6261aea5930e","execution":{"iopub.status.busy":"2021-12-01T07:59:30.719952Z","iopub.execute_input":"2021-12-01T07:59:30.720261Z","iopub.status.idle":"2021-12-01T07:59:33.516201Z","shell.execute_reply.started":"2021-12-01T07:59:30.720221Z","shell.execute_reply":"2021-12-01T07:59:33.515432Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Calculating total fare amount values at each percntile 0 to 100 \n\nfor i in range(0,100,10):\n    var = frame_with_durations_modified[\"total_amount\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"cI2KC9QipYTo","outputId":"69b71990-e810-4078-9be6-138323edaf1f","execution":{"iopub.status.busy":"2021-12-01T07:59:33.517419Z","iopub.execute_input":"2021-12-01T07:59:33.517666Z","iopub.status.idle":"2021-12-01T07:59:41.962597Z","shell.execute_reply.started":"2021-12-01T07:59:33.517639Z","shell.execute_reply":"2021-12-01T07:59:41.961720Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Calculating total fare amount values at each percntile 90,91...99,100\n\nfor i in range(90,100):\n    var = frame_with_durations_modified[\"total_amount\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"_i7OiuGLpYTr","outputId":"595fc288-2f41-47e6-c0f5-f44912c35df7","execution":{"iopub.status.busy":"2021-12-01T07:59:41.963607Z","iopub.execute_input":"2021-12-01T07:59:41.963809Z","iopub.status.idle":"2021-12-01T07:59:50.402815Z","shell.execute_reply.started":"2021-12-01T07:59:41.963777Z","shell.execute_reply":"2021-12-01T07:59:50.402018Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Calculating total fare amount values at each percntile 99.0,99.1.....100\n\nfor i in np.arange(0.0, 1.0, 0.1):\n    var = frame_with_durations_modified[\"total_amount\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)/100))]))\nprint(\"100 percentile value is \",var[-1])","metadata":{"id":"qezpzkQEpYTt","outputId":"d5adc780-a7be-4964-a83b-fcd513f7c1ae","execution":{"iopub.status.busy":"2021-12-01T07:59:50.403932Z","iopub.execute_input":"2021-12-01T07:59:50.404159Z","iopub.status.idle":"2021-12-01T07:59:58.882012Z","shell.execute_reply.started":"2021-12-01T07:59:50.404131Z","shell.execute_reply":"2021-12-01T07:59:58.881102Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"<b>Observation:-</b> As even the 99.9th percentile value doesnt look like an outlier,as there is not much difference between the 99.8th percentile and 99.9th percentile, we move on to do graphical analyis","metadata":{"id":"lC4YdJFxpYTv"}},{"cell_type":"code","source":"# Below plot shows us the fare values(sorted) to find a sharp increase to remove those values as outliers\n# plot the fare amount excluding last two values in sorted data\n\nplt.plot(var[:-2])\nplt.show()","metadata":{"id":"foR7D-kCpYTw","outputId":"1bec5fe0-8950-4459-dd90-17e3b58468a1","execution":{"iopub.status.busy":"2021-12-01T07:59:58.883106Z","iopub.execute_input":"2021-12-01T07:59:58.883554Z","iopub.status.idle":"2021-12-01T08:00:00.415813Z","shell.execute_reply.started":"2021-12-01T07:59:58.883522Z","shell.execute_reply":"2021-12-01T08:00:00.414920Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# A very sharp increase in fare values can be seen \n# plotting last three total fare values, and we can observe there is share increase in the values\n\nplt.plot(var[-3:])\nplt.show()","metadata":{"id":"l342yQzhpYTz","outputId":"6bdee300-ea3e-47fc-edaf-1bf5ab902b2a","execution":{"iopub.status.busy":"2021-12-01T08:00:00.417221Z","iopub.execute_input":"2021-12-01T08:00:00.417548Z","iopub.status.idle":"2021-12-01T08:00:00.614234Z","shell.execute_reply.started":"2021-12-01T08:00:00.417509Z","shell.execute_reply":"2021-12-01T08:00:00.613469Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Now looking at values not including the last two points we again find a drastic increase at around 1000 fare value\n# we plot last 50 values excluding last two values\n\nplt.plot(var[-50:-2])\nplt.show()","metadata":{"id":"gr1X5fvlpYT4","outputId":"4ba61efc-0446-4c89-b26d-4f8a2d154a55","execution":{"iopub.status.busy":"2021-12-01T08:00:00.615920Z","iopub.execute_input":"2021-12-01T08:00:00.616164Z","iopub.status.idle":"2021-12-01T08:00:00.785019Z","shell.execute_reply.started":"2021-12-01T08:00:00.616136Z","shell.execute_reply":"2021-12-01T08:00:00.784039Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Remove all outliers/erronous points.","metadata":{"id":"HlvWCK4PpYT7"}},{"cell_type":"code","source":"# Removing all outliers based on our univariate analysis above\n\ndef remove_outliers(new_frame):\n    \n    a = new_frame.shape[0]\n    print (\"Number of pickup records = \",a)\n    temp_frame = new_frame[((new_frame.dropoff_longitude >= -74.15) & (new_frame.dropoff_longitude <= -73.7004) &\\\n                       (new_frame.dropoff_latitude >= 40.5774) & (new_frame.dropoff_latitude <= 40.9176)) & \\\n                       ((new_frame.pickup_longitude >= -74.15) & (new_frame.pickup_latitude >= 40.5774)& \\\n                       (new_frame.pickup_longitude <= -73.7004) & (new_frame.pickup_latitude <= 40.9176))]\n    b = temp_frame.shape[0]\n    print (\"Number of outlier coordinates lying outside NY boundaries:\",(a-b))\n\n    \n    temp_frame = new_frame[(new_frame.trip_times > 0) & (new_frame.trip_times < 720)]\n    c = temp_frame.shape[0]\n    print (\"Number of outliers from trip times analysis:\",(a-c))\n    \n    \n    temp_frame = new_frame[(new_frame.trip_distance > 0) & (new_frame.trip_distance < 23)]\n    d = temp_frame.shape[0]\n    print (\"Number of outliers from trip distance analysis:\",(a-d))\n    \n    temp_frame = new_frame[(new_frame.Speed <= 65) & (new_frame.Speed >= 0)]\n    e = temp_frame.shape[0]\n    print (\"Number of outliers from speed analysis:\",(a-e))\n    \n    temp_frame = new_frame[(new_frame.total_amount <1000) & (new_frame.total_amount >0)]\n    f = temp_frame.shape[0]\n    print (\"Number of outliers from fare analysis:\",(a-f))\n    \n    \n    new_frame = new_frame[((new_frame.dropoff_longitude >= -74.15) & (new_frame.dropoff_longitude <= -73.7004) &\\\n                       (new_frame.dropoff_latitude >= 40.5774) & (new_frame.dropoff_latitude <= 40.9176)) & \\\n                       ((new_frame.pickup_longitude >= -74.15) & (new_frame.pickup_latitude >= 40.5774)& \\\n                       (new_frame.pickup_longitude <= -73.7004) & (new_frame.pickup_latitude <= 40.9176))]\n    \n    new_frame = new_frame[(new_frame.trip_times > 0) & (new_frame.trip_times < 720)]\n    new_frame = new_frame[(new_frame.trip_distance > 0) & (new_frame.trip_distance < 23)]\n    new_frame = new_frame[(new_frame.Speed < 45.31) & (new_frame.Speed > 0)]\n    new_frame = new_frame[(new_frame.total_amount <1000) & (new_frame.total_amount >0)]\n    \n    print (\"Total outliers removed\",a - new_frame.shape[0])\n    print (\"---------\")\n    return new_frame","metadata":{"id":"SPORvXJHpYT8","execution":{"iopub.status.busy":"2021-12-01T08:00:00.786461Z","iopub.execute_input":"2021-12-01T08:00:00.786802Z","iopub.status.idle":"2021-12-01T08:00:00.799335Z","shell.execute_reply.started":"2021-12-01T08:00:00.786762Z","shell.execute_reply":"2021-12-01T08:00:00.798331Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print (\"Removing outliers in the month of Jan-2015\")\nprint (\"------------\")\nframe_with_durations_outliers_removed = remove_outliers(frame_with_durations)\nprint(\"fraction of data points that remain after removing outliers\", float(len(frame_with_durations_outliers_removed))/len(frame_with_durations))","metadata":{"id":"gLt_0naFpYT-","outputId":"b9346ed9-12b2-4c9e-f6a4-c7386fe13f53","execution":{"iopub.status.busy":"2021-12-01T08:00:00.800581Z","iopub.execute_input":"2021-12-01T08:00:00.800804Z","iopub.status.idle":"2021-12-01T08:00:07.536635Z","shell.execute_reply.started":"2021-12-01T08:00:00.800777Z","shell.execute_reply":"2021-12-01T08:00:07.535763Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#### Observation:\nWe just lost 3% of data after remving all outliers. We can build a model on remaining 97% of the data","metadata":{}},{"cell_type":"markdown","source":"# Data-preperation\n## Clustering/Segmentation","metadata":{"id":"wiSR_ZaspYUA"}},{"cell_type":"code","source":"# Trying different cluster sizes to choose the right K in K-means\n\ncoords = frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']].values\nneighbours=[]\n\ndef find_min_distance(cluster_centers, cluster_len):\n    nice_points = 0\n    wrong_points = 0\n    less2 = []\n    more2 = []\n    min_dist=1000\n    for i in range(0, cluster_len):\n        nice_points = 0\n        wrong_points = 0\n        for j in range(0, cluster_len):\n            if j!=i:\n                distance = gpxpy.geo.haversine_distance(cluster_centers[i][0], cluster_centers[i][1],cluster_centers[j][0], cluster_centers[j][1])\n                min_dist = min(min_dist,distance/(1.60934*1000))\n                \n                if (distance/(1.60934*1000)) <= 2:\n                    nice_points +=1\n                else:\n                    wrong_points += 1\n        less2.append(nice_points)\n        more2.append(wrong_points)\n    neighbours.append(less2)\n    print (\"On choosing a cluster size of \",cluster_len,\"\\nAvg. Number of Clusters within the vicinity (i.e. intercluster-distance < 2):\", np.ceil(sum(less2)/len(less2)), \"\\nAvg. Number of Clusters outside the vicinity (i.e. intercluster-distance > 2):\", np.ceil(sum(more2)/len(more2)),\"\\nMin inter-cluster distance = \",min_dist,\"\\n---\")\n\ndef find_clusters(increment):\n    kmeans = MiniBatchKMeans(n_clusters=increment, batch_size=10000,random_state=42).fit(coords)\n    frame_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n    cluster_centers = kmeans.cluster_centers_\n    cluster_len = len(cluster_centers)\n    return cluster_centers, cluster_len\n\n# We need to choose number of clusters so that, there are more number of cluster regions \n# that are close to any cluster center and make sure that the minimum inter cluster should not be very less\nfor increment in range(10, 100, 10):\n    cluster_centers, cluster_len = find_clusters(increment)\n    find_min_distance(cluster_centers, cluster_len)            ","metadata":{"id":"bdy3QNcupYUB","outputId":"45e00b98-477e-415d-d1c7-fa5fc6c230f0","execution":{"iopub.status.busy":"2021-12-01T08:00:07.537777Z","iopub.execute_input":"2021-12-01T08:00:07.537995Z","iopub.status.idle":"2021-12-01T08:00:58.051240Z","shell.execute_reply.started":"2021-12-01T08:00:07.537969Z","shell.execute_reply":"2021-12-01T08:00:58.050590Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n- The main objective was to find a optimal min. distance(Which roughly estimates to the radius of a cluster) between the clusters which we got was 40\n\n- If check for the 50 clusters you can observe that there are two clusters with only 0.3 miles apart from each other therefore, I choosed 40 clusters for solve the further problem","metadata":{"id":"9w0v8KAzpYUE"}},{"cell_type":"code","source":"# Getting 40 clusters using the k-means \n\nkmeans = MiniBatchKMeans(n_clusters=40, batch_size=10000,random_state=0).fit(coords)\nframe_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])","metadata":{"id":"U9hu-gmfpYUF","execution":{"iopub.status.busy":"2021-12-01T08:00:58.052558Z","iopub.execute_input":"2021-12-01T08:00:58.053054Z","iopub.status.idle":"2021-12-01T08:01:03.935560Z","shell.execute_reply.started":"2021-12-01T08:00:58.053017Z","shell.execute_reply":"2021-12-01T08:01:03.934846Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the cluster centers","metadata":{"id":"5i2xz9z_pYUI"}},{"cell_type":"code","source":"# Plotting the cluster centers on OSM\n\ncluster_centers = kmeans.cluster_centers_\ncluster_len = len(cluster_centers)\nmap_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\nfor i in range(cluster_len):\n    folium.Marker(list((cluster_centers[i][0],cluster_centers[i][1])), popup=(str(cluster_centers[i][0])+str(cluster_centers[i][1]))).add_to(map_osm)\nmap_osm","metadata":{"id":"GNUtiOAGpYUJ","outputId":"960ac46a-3fca-4bd3-ed3d-f11f01f751a6","execution":{"iopub.status.busy":"2021-12-01T08:01:03.939623Z","iopub.execute_input":"2021-12-01T08:01:03.941909Z","iopub.status.idle":"2021-12-01T08:01:04.026041Z","shell.execute_reply.started":"2021-12-01T08:01:03.941861Z","shell.execute_reply":"2021-12-01T08:01:04.025363Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the clusters","metadata":{"id":"hwjprgJipYUM"}},{"cell_type":"code","source":"# Visualising the clusters on a map\n\ndef plot_clusters(frame):\n    city_long_border = (-74.03, -73.75)\n    city_lat_border = (40.63, 40.85)\n    fig, ax = plt.subplots(ncols=1, nrows=1)\n    ax.scatter(frame.pickup_longitude.values[:100000], frame.pickup_latitude.values[:100000], s=10, lw=0,\n               c=frame.pickup_cluster.values[:100000], cmap='tab20', alpha=0.2)\n    ax.set_xlim(city_long_border)\n    ax.set_ylim(city_lat_border)\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    plt.show()\n\nplot_clusters(frame_with_durations_outliers_removed)","metadata":{"id":"1xrUz8KRpYUN","outputId":"d2b37409-8654-4f58-91bd-30cce95dac8a","execution":{"iopub.status.busy":"2021-12-01T08:01:04.030320Z","iopub.execute_input":"2021-12-01T08:01:04.032666Z","iopub.status.idle":"2021-12-01T08:01:04.690148Z","shell.execute_reply.started":"2021-12-01T08:01:04.032620Z","shell.execute_reply":"2021-12-01T08:01:04.689347Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Time-binning\n Refer:https://www.unixtimestamp.com/\n- 1420070400 : 2015-01-01 00:00:00 \n- 1422748800 : 2015-02-01 00:00:00 \n- 1425168000 : 2015-03-01 00:00:00\n- 1427846400 : 2015-04-01 00:00:00 \n- 1430438400 : 2015-05-01 00:00:00 \n- 1433116800 : 2015-06-01 00:00:00\n\n- 1451606400 : 2016-01-01 00:00:00 \n- 1454284800 : 2016-02-01 00:00:00 \n- 1456790400 : 2016-03-01 00:00:00\n- 1459468800 : 2016-04-01 00:00:00 \n- 1462060800 : 2016-05-01 00:00:00 \n- 1464739200 : 2016-06-01 00:00:00","metadata":{"id":"r-RK9PzspYUP"}},{"cell_type":"code","source":"def add_pickup_bins(frame,month,year):\n    unix_pickup_times=[i for i in frame['pickup_times'].values]\n    unix_times = [[1420070400,1422748800,1425168000,1427846400,1430438400,1433116800],\\\n                    [1451606400,1454284800,1456790400,1459468800,1462060800,1464739200]]\n    \n    start_pickup_unix=unix_times[year-2015][month-1]\n\n    # (int((i-start_pickup_unix)/600)+33) : our unix time is in gmt to we are converting it to est\n    \n    tenminutewise_binned_unix_pickup_times=[(int((i-start_pickup_unix)/600)+33) for i in unix_pickup_times]\n    frame['pickup_bins'] = np.array(tenminutewise_binned_unix_pickup_times)\n    return frame","metadata":{"id":"WhGxh1VXpYUQ","execution":{"iopub.status.busy":"2021-12-01T08:01:04.691667Z","iopub.execute_input":"2021-12-01T08:01:04.692148Z","iopub.status.idle":"2021-12-01T08:01:04.700095Z","shell.execute_reply.started":"2021-12-01T08:01:04.692103Z","shell.execute_reply":"2021-12-01T08:01:04.698942Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Clustering, making pickup bins and grouping by pickup cluster and pickup bins\n\nframe_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])\njan_2015_frame = add_pickup_bins(frame_with_durations_outliers_removed,1,2015)\njan_2015_groupby = jan_2015_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()","metadata":{"id":"9bO_QAAGpYUR","execution":{"iopub.status.busy":"2021-12-01T08:01:04.706166Z","iopub.execute_input":"2021-12-01T08:01:04.706506Z","iopub.status.idle":"2021-12-01T08:01:20.853621Z","shell.execute_reply.started":"2021-12-01T08:01:04.706467Z","shell.execute_reply":"2021-12-01T08:01:20.852745Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# we add two more columns 'pickup_cluster'(to which cluster it belogns to) and 'pickup_bins' (to which 10min intravel the trip belongs to)\njan_2015_frame.head()","metadata":{"id":"Ne-s1QW_pYUT","outputId":"b49abc6d-1e6e-40f2-cf47-663a2f1d6e48","execution":{"iopub.status.busy":"2021-12-01T08:01:20.854753Z","iopub.execute_input":"2021-12-01T08:01:20.855044Z","iopub.status.idle":"2021-12-01T08:01:20.871476Z","shell.execute_reply.started":"2021-12-01T08:01:20.855002Z","shell.execute_reply":"2021-12-01T08:01:20.870551Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### Here the trip_distance represents the number of pickups that are happend in that particular 10 mins intervel\n\n- This data frame has two indices\n- primary index: pickup_cluster (cluster number)\n- secondary index : pickup_bins (we divide whole months time into 10 min intravels 24*31*60/10 = 4464bins)\n","metadata":{}},{"cell_type":"code","source":"jan_2015_groupby.head()","metadata":{"id":"Q56tfNUmpYUW","outputId":"9dda59f4-59c0-4557-e4fd-9ce226d98225","execution":{"iopub.status.busy":"2021-12-01T08:01:20.872899Z","iopub.execute_input":"2021-12-01T08:01:20.873479Z","iopub.status.idle":"2021-12-01T08:01:20.889542Z","shell.execute_reply.started":"2021-12-01T08:01:20.873425Z","shell.execute_reply":"2021-12-01T08:01:20.888910Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"#### Till now we cleaned data and prepared data for the month 2015, Now do the same operations for months Jan, Feb, March of 2016\n1. Get the dataframe which inlcudes only required colums\n2. Adding trip times, speed, unix time stamp of pickup_time\n3. Remove the outliers based on trip_times, speed, trip_duration, total_amount\n4. Add pickup_cluster to each data point\n5. Add pickup_bin (index of 10min intravel to which that trip belongs to)\n6. Group by data, based on 'pickup_cluster' and 'pickuo_bin'","metadata":{}},{"cell_type":"code","source":"# Data Preparation for the months of Jan,Feb and March 2016\n\ndef datapreparation(month,kmeans,month_no,year_no):\n    \n    print (\"Return with trip times..\")\n\n    frame_with_durations = return_with_trip_times(month)\n    \n    print (\"Remove outliers..\")\n    frame_with_durations_outliers_removed = remove_outliers(frame_with_durations)\n    \n    print (\"Estimating clusters..\")\n    frame_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n    #frame_with_durations_outliers_removed_2016['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed_2016[['pickup_latitude', 'pickup_longitude']])\n\n    print (\"Final groupbying..\")\n    final_updated_frame = add_pickup_bins(frame_with_durations_outliers_removed,month_no,year_no)\n    final_groupby_frame = final_updated_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()\n    \n    return final_updated_frame,final_groupby_frame\n    \nmonth_jan_2016 = dd.read_csv('../input/newyork-taxi-demand/yellow_tripdata_2016-01.csv')\nmonth_feb_2016 = dd.read_csv('../input/newyork-taxi-demand/yellow_tripdata_2016-02.csv')\nmonth_mar_2016 = dd.read_csv('../input/newyork-taxi-demand/yellow_tripdata_2016-03.csv')\n\njan_2016_frame,jan_2016_groupby = datapreparation(month_jan_2016,kmeans,1,2016)\nfeb_2016_frame,feb_2016_groupby = datapreparation(month_feb_2016,kmeans,2,2016)\nmar_2016_frame,mar_2016_groupby = datapreparation(month_mar_2016,kmeans,3,2016)","metadata":{"id":"sLMjW_depYUa","outputId":"f2bbb0cd-e103-4168-e07f-5da5267fec9a","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T08:01:20.890758Z","iopub.execute_input":"2021-12-01T08:01:20.891278Z","iopub.status.idle":"2021-12-01T08:24:30.519747Z","shell.execute_reply.started":"2021-12-01T08:01:20.891246Z","shell.execute_reply":"2021-12-01T08:24:30.515285Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Smoothing\n\n- Gets the unique bins where pickup values are present for each each reigion\n- For each cluster region we will collect all the indices of 10min intravels in which the pickups are happened we got an observation that there are some pickpbins that doesnt have any pickups","metadata":{"id":"HEW9vuBwpYUc"}},{"cell_type":"code","source":"def return_unq_pickup_bins(frame):\n    values = []\n    for i in range(0,40):\n        new = frame[frame['pickup_cluster'] == i]\n        list_unq = list(set(new['pickup_bins']))\n        list_unq.sort()\n        values.append(list_unq)\n    return values","metadata":{"id":"rUMSCEHEpYUd","execution":{"iopub.status.busy":"2021-12-01T08:24:30.523048Z","iopub.execute_input":"2021-12-01T08:24:30.523724Z","iopub.status.idle":"2021-12-01T08:24:30.535888Z","shell.execute_reply.started":"2021-12-01T08:24:30.523682Z","shell.execute_reply":"2021-12-01T08:24:30.533998Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# For every month we get all indices of 10min intravels in which atleast one pickup got happened\n\n# Jan\njan_2015_unique = return_unq_pickup_bins(jan_2015_frame)\njan_2016_unique = return_unq_pickup_bins(jan_2016_frame)\n\n# Feb\nfeb_2016_unique = return_unq_pickup_bins(feb_2016_frame)\n\n# March\nmar_2016_unique = return_unq_pickup_bins(mar_2016_frame)","metadata":{"id":"VeYcUCUxpYUf","execution":{"iopub.status.busy":"2021-12-01T08:24:30.537058Z","iopub.execute_input":"2021-12-01T08:24:30.537287Z","iopub.status.idle":"2021-12-01T08:24:47.387366Z","shell.execute_reply.started":"2021-12-01T08:24:30.537259Z","shell.execute_reply":"2021-12-01T08:24:47.386455Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# For each cluster number of 10min intravels with 0 pickups\n\nfor i in range(40):\n    print(\"for the \",i,\"th cluster number of 10min intavels with zero pickups: \",4464 - len(set(jan_2015_unique[i])))\n    print('-'*60)","metadata":{"id":"Z5aZogYEpYUg","outputId":"fb3386a0-2a07-4ab1-ba84-8cd014d8d1c3","execution":{"iopub.status.busy":"2021-12-01T08:24:47.388539Z","iopub.execute_input":"2021-12-01T08:24:47.388768Z","iopub.status.idle":"2021-12-01T08:24:47.431390Z","shell.execute_reply.started":"2021-12-01T08:24:47.388740Z","shell.execute_reply":"2021-12-01T08:24:47.430756Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"There are two ways to fill up these values\n<ul>\n<li> Fill the missing value with 0's</li>\n<li> Fill the missing values with the avg values\n<ul>\n<li> Case 1:(values missing at the start)  <br>Ex1: \\_ \\_ \\_ x =>ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4) <br> Ex2: \\_ \\_ x => ceil(x/3), ceil(x/3), ceil(x/3) </li>\n<li> Case 2:(values missing in middle) <br>Ex1: x \\_ \\_ y => ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4) <br> Ex2: x \\_ \\_ \\_ y => ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5) </li>\n<li> Case 3:(values missing at the end)  <br>Ex1: x \\_ \\_ \\_  => ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4) <br> Ex2: x \\_  => ceil(x/2), ceil(x/2) </li>\n</ul>\n</li>\n</ul>","metadata":{"id":"-Ocy5bDipYUj"}},{"cell_type":"markdown","source":"#### Fills a value of zero for every bin where no pickup data is present the count_values: number pickps that are happened in each region for each 10min intravel there wont be any value if there are no pickups. \n- values: number of unique bins\n- For every 10min intervel(pickup_bin) we will check it is there in our unique bin, if it is there we will add the count_values[index] to smoothed data if not we add 0 to the smoothed data we finally return smoothed data","metadata":{}},{"cell_type":"code","source":"def fill_missing(count_values,values):\n    smoothed_regions=[]\n    ind=0\n    for r in range(0,40):\n        smoothed_bins=[]\n        for i in range(4464):\n            if i in values[r]:\n                smoothed_bins.append(count_values[ind])\n                ind+=1\n            else:\n                smoothed_bins.append(0)\n        smoothed_regions.extend(smoothed_bins)\n    return smoothed_regions","metadata":{"id":"B84diUuKpYUj","execution":{"iopub.status.busy":"2021-12-01T08:24:47.432554Z","iopub.execute_input":"2021-12-01T08:24:47.432840Z","iopub.status.idle":"2021-12-01T08:24:47.439604Z","shell.execute_reply.started":"2021-12-01T08:24:47.432800Z","shell.execute_reply":"2021-12-01T08:24:47.438666Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"#### Fills a value of zero for every bin where no pickup data is present the count_values: number pickps that are happened in each region for each 10min intervel there wont be any value if there are no picksups.\n- values: number of unique bins\n\n- For every 10min intravel(pickup_bin) we will check it is there in our unique bin, if it is there we will add the count_values[index] to smoothed data if not we add smoothed data we finally return smoothed data","metadata":{}},{"cell_type":"code","source":"def smoothing(count_values,values):\n    smoothed_regions=[] # stores list of final smoothed values of each reigion\n    ind=0\n    repeat=0 \n    smoothed_value=0\n    for r in range(0,40):\n        smoothed_bins=[] #stores the final smoothed values\n        repeat=0\n        for i in range(4464):\n            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n                repeat-=1\n                continue\n            if i in values[r]: #checks if the pickup-bin exists \n                smoothed_bins.append(count_values[ind]) # appends the value of the pickup bin if it exists\n            else:\n                if i!=0:\n                    right_hand_limit=0\n                    for j in range(i,4464):\n                        if  j not in values[r]: #searches for the left-limit or the pickup-bin value which has a pickup value\n                            continue\n                        else:\n                            right_hand_limit=j\n                            break\n                    if right_hand_limit==0:\n                    # Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n                        \n                        smoothed_value=count_values[ind-1]*1.0/((4463-i)+2)*1.0                               \n                        for j in range(i,4464):                              \n                            smoothed_bins.append(math.ceil(smoothed_value))\n                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n                        repeat=(4463-i)\n                        ind-=1\n                    else:\n                    # Case 2: When we have the missing values between two known values\n                        \n                        smoothed_value=(count_values[ind-1]+count_values[ind])*1.0/((right_hand_limit-i)+2)*1.0             \n                        for j in range(i,right_hand_limit+1):\n                            smoothed_bins.append(math.ceil(smoothed_value))\n                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n                        repeat=(right_hand_limit-i)\n                else:\n                # Case 3: When we have the first/first few values are found to be missing,hence we have no left-limit here\n                    \n                    right_hand_limit=0\n                    for j in range(i,4464):\n                        if  j not in values[r]:\n                            continue\n                        else:\n                            right_hand_limit=j\n                            break\n                    smoothed_value=count_values[ind]*1.0/((right_hand_limit-i)+1)*1.0\n                    for j in range(i,right_hand_limit+1):\n                            smoothed_bins.append(math.ceil(smoothed_value))\n                    repeat=(right_hand_limit-i)\n            ind+=1\n        smoothed_regions.extend(smoothed_bins)\n    return smoothed_regions","metadata":{"id":"qtwtQnhQpYUl","execution":{"iopub.status.busy":"2021-12-01T08:24:47.440938Z","iopub.execute_input":"2021-12-01T08:24:47.441253Z","iopub.status.idle":"2021-12-01T08:24:47.454152Z","shell.execute_reply.started":"2021-12-01T08:24:47.441224Z","shell.execute_reply":"2021-12-01T08:24:47.453338Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#Filling Missing values of Jan-2015 with 0\n# here in jan_2015_groupby dataframe the trip_distance represents the number of pickups that are happened\n\njan_2015_fill = fill_missing(jan_2015_groupby['trip_distance'].values,jan_2015_unique)\n\n\n#Smoothing Missing values of Jan-2015\n\njan_2015_smooth = smoothing(jan_2015_groupby['trip_distance'].values,jan_2015_unique)","metadata":{"id":"cGn9mQ-UpYUn","execution":{"iopub.status.busy":"2021-12-01T08:24:47.456654Z","iopub.execute_input":"2021-12-01T08:24:47.457313Z","iopub.status.idle":"2021-12-01T08:24:56.289739Z","shell.execute_reply.started":"2021-12-01T08:24:47.457277Z","shell.execute_reply":"2021-12-01T08:24:56.288876Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# for each cluster we will have 4464 values, therefore 40*4464 = 178560 (length of the jan_2015_fill)\n\nprint(\"number of 10min intravels among all the clusters \",len(jan_2015_fill))","metadata":{"id":"gh1VGbXUpYUp","outputId":"825770f3-c6a3-428e-c5a6-8531d06cab7f","execution":{"iopub.status.busy":"2021-12-01T08:24:56.291458Z","iopub.execute_input":"2021-12-01T08:24:56.292236Z","iopub.status.idle":"2021-12-01T08:24:56.298613Z","shell.execute_reply.started":"2021-12-01T08:24:56.292188Z","shell.execute_reply":"2021-12-01T08:24:56.297628Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Smoothing vs Filling\n# sample plot that shows two variations of filling missing values\n# we have taken the number of pickups for cluster region 2\n\nplt.figure(figsize=(10,5))\nplt.plot(jan_2015_fill[4464:8920], label=\"zero filled values\")\nplt.plot(jan_2015_smooth[4464:8920], label=\"filled with avg values\")\nplt.legend()\nplt.show()","metadata":{"id":"QrJ2Maw_pYUr","outputId":"270ad648-f3fd-441f-b29e-8b4d774a4c5b","execution":{"iopub.status.busy":"2021-12-01T08:46:35.176125Z","iopub.execute_input":"2021-12-01T08:46:35.176556Z","iopub.status.idle":"2021-12-01T08:46:35.420045Z","shell.execute_reply.started":"2021-12-01T08:46:35.176516Z","shell.execute_reply":"2021-12-01T08:46:35.419174Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"#### I choose, these methods and used for only 2015 data and not on 2016 data beacause -\n\n- Consider we have data of some month in 2015 jan 1st, 10 _ _ _ 20, i.e there are 10 pickups that are happened in 1st 10min intravel, 0 pickups happened in 2nd 10mins intravel, 0 pickups happened in 3rd 10min intravel and 20 pickups happened in 4th 10min intravel in fill_missing method we replace these values like 10, 0, 0, 20\n- Where as in smoothing method we replace these values as 6,6,6,6,6, if you can check the number of pickups that are happened in the first 40min are same in both cases, but if you can observe that we looking at the future values \n- When you are using smoothing we are looking at the future number of pickups which might cause a data leakage. Therefore I use smoothing for jan 2015th data since it acts as our training data and we use simple fill_misssing method for 2016th data.","metadata":{"id":"juSevxU_pYUu","execution":{"iopub.status.busy":"2021-12-01T08:24:56.574138Z","iopub.execute_input":"2021-12-01T08:24:56.575042Z","iopub.status.idle":"2021-12-01T08:24:56.578285Z","shell.execute_reply.started":"2021-12-01T08:24:56.575003Z","shell.execute_reply":"2021-12-01T08:24:56.577745Z"}}},{"cell_type":"code","source":"# Jan-2015 data is smoothed, Jan,Feb & March 2016 data missing values are filled with zero\njan_2015_smooth = smoothing(jan_2015_groupby['trip_distance'].values,jan_2015_unique)\njan_2016_smooth = fill_missing(jan_2016_groupby['trip_distance'].values,jan_2016_unique)\nfeb_2016_smooth = fill_missing(feb_2016_groupby['trip_distance'].values,feb_2016_unique)\nmar_2016_smooth = fill_missing(mar_2016_groupby['trip_distance'].values,mar_2016_unique)\n\n# Making list of all the values of pickup data in every bin for a period of 3 months and storing them region-wise \nregions_cum = []\n\n# number of 10min indices for jan 2015= 24*31*60/10 = 4464\n# number of 10min indices for jan 2016 = 24*31*60/10 = 4464\n# number of 10min indices for feb 2016 = 24*29*60/10 = 4176\n# number of 10min indices for march 2016 = 24*31*60/10 = 4464\n# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n# that are happened for three months in 2016 data\n\nfor i in range(0,40):\n    regions_cum.append(jan_2016_smooth[4464*i:4464*(i+1)]+feb_2016_smooth[4176*i:4176*(i+1)]+mar_2016_smooth[4464*i:4464*(i+1)])","metadata":{"id":"EHGsXfb0pYUz","execution":{"iopub.status.busy":"2021-12-01T08:24:56.579453Z","iopub.execute_input":"2021-12-01T08:24:56.580125Z","iopub.status.idle":"2021-12-01T08:25:14.215738Z","shell.execute_reply.started":"2021-12-01T08:24:56.580090Z","shell.execute_reply":"2021-12-01T08:25:14.214811Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Time series and Fourier Transforms","metadata":{"id":"XnipIdfYpYU0"}},{"cell_type":"code","source":"def uniqueish_color():\n    \"\"\"There're better ways to generate unique colors, but this isn't awful.\"\"\"\n    return plt.cm.gist_ncar(np.random.random())\nfirst_x = list(range(0,4464))\nsecond_x = list(range(4464,8640))\nthird_x = list(range(8640,13104))\nfor i in range(40):\n    plt.figure(figsize=(10,4))\n    plt.plot(first_x,regions_cum[i][:4464], color=uniqueish_color(), label='2016 Jan month data')\n    plt.plot(second_x,regions_cum[i][4464:8640], color=uniqueish_color(), label='2016 feb month data')\n    plt.plot(third_x,regions_cum[i][8640:], color=uniqueish_color(), label='2016 march month data')\n    plt.legend()\n    plt.show()","metadata":{"id":"ud1K-HcZpYU2","outputId":"506387e6-b606-4c9d-f956-f5691301db45","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T08:25:14.217304Z","iopub.execute_input":"2021-12-01T08:25:14.218135Z","iopub.status.idle":"2021-12-01T08:25:24.873130Z","shell.execute_reply.started":"2021-12-01T08:25:14.218082Z","shell.execute_reply":"2021-12-01T08:25:24.872099Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# getting peaks: https://blog.ytotech.com/2015/11/01/findpeaks-in-python/\n# read more about fft function : https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html\nY    = np.fft.fft(np.array(jan_2016_smooth)[0:4460])\n# read more about the fftfreq: https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftfreq.html  \nfreq = np.fft.fftfreq(4460, 1)\nn = len(freq)\nplt.figure()\nplt.plot( freq[:int(n/2)], np.abs(Y)[:int(n/2)] )\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Amplitude\")\nplt.show()","metadata":{"id":"76vu5IXCpYU8","outputId":"ada73f2d-ff31-4d62-f6d9-ef79d41db650","execution":{"iopub.status.busy":"2021-12-01T08:25:24.874734Z","iopub.execute_input":"2021-12-01T08:25:24.875139Z","iopub.status.idle":"2021-12-01T08:25:25.097446Z","shell.execute_reply.started":"2021-12-01T08:25:24.875090Z","shell.execute_reply":"2021-12-01T08:25:25.096566Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#Preparing the Dataframe only with x(i) values as jan-2015 data and y(i) values as jan-2016\nratios_jan = pd.DataFrame()\nratios_jan['Given']=jan_2015_smooth\nratios_jan['Prediction']=jan_2016_smooth\nratios_jan['Ratios']=ratios_jan['Prediction']*1.0/ratios_jan['Given']*1.0","metadata":{"id":"J9iVFeu0pYU-","execution":{"iopub.status.busy":"2021-12-01T08:25:25.098933Z","iopub.execute_input":"2021-12-01T08:25:25.099150Z","iopub.status.idle":"2021-12-01T08:25:25.254277Z","shell.execute_reply.started":"2021-12-01T08:25:25.099125Z","shell.execute_reply":"2021-12-01T08:25:25.253502Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Modelling: Baseline Models\n\nNow we get into modelling in order to forecast the pickup densities for the months of Jan, Feb and March of 2016 for which we are using multiple models with two variations \n1. Using Ratios of the 2016 data to the 2015 data i.e $\\begin{align} R_{t} = P^{2016}_{t} / P^{2015}_{t} \\end{align}$\n2. Using Previous known values of the 2016 data itself to predict the future values","metadata":{"id":"e7hsq8chpYU_"}},{"cell_type":"markdown","source":"### Simple Moving Averages\nThe First Model used is the Moving Averages Model which uses the previous n values in order to predict the next value <br> ","metadata":{"id":"NomEUpKJpYVA"}},{"cell_type":"markdown","source":"Using Ratio Values - $\\begin{align}R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3} .... R_{t-n} )/n \\end{align}$","metadata":{"id":"KiXJynDKpYVB"}},{"cell_type":"code","source":"def MA_R_Predictions(ratios,month):\n    predicted_ratio=(ratios['Ratios'].values)[0]\n    error=[]\n    predicted_values=[]\n    window_size=3\n    predicted_ratio_values=[]\n    for i in range(0,4464*40):\n        if i%4464==0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(int(((ratios['Given'].values)[i])*predicted_ratio))\n        error.append(abs((math.pow(int(((ratios['Given'].values)[i])*predicted_ratio)-(ratios['Prediction'].values)[i],1))))\n        if i+1>=window_size:\n            predicted_ratio=sum((ratios['Ratios'].values)[(i+1)-window_size:(i+1)])/window_size\n        else:\n            predicted_ratio=sum((ratios['Ratios'].values)[0:(i+1)])/(i+1)\n            \n    \n    ratios['MA_R_Predicted'] = predicted_values\n    ratios['MA_R_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"5fvHpsEbpYVB","execution":{"iopub.status.busy":"2021-12-01T08:25:25.255333Z","iopub.execute_input":"2021-12-01T08:25:25.256019Z","iopub.status.idle":"2021-12-01T08:25:25.265813Z","shell.execute_reply.started":"2021-12-01T08:25:25.255983Z","shell.execute_reply":"2021-12-01T08:25:25.264766Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 3 is optimal for getting the best results using Moving Averages using previous Ratio values therefore we get $\\begin{align}R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3})/3 \\end{align}$","metadata":{"id":"B761jWmJpYVH"}},{"cell_type":"markdown","source":"Next we use the Moving averages of the 2016  values itself to predict the future value using $\\begin{align}P_{t} = ( P_{t-1} + P_{t-2} + P_{t-3} .... P_{t-n} )/n \\end{align}$","metadata":{"id":"uDl5QIpwpYVI"}},{"cell_type":"code","source":"def MA_P_Predictions(ratios,month):\n    predicted_value=(ratios['Prediction'].values)[0]\n    error=[]\n    predicted_values=[]\n    window_size=1\n    predicted_ratio_values=[]\n    for i in range(0,4464*40):\n        predicted_values.append(predicted_value)\n        error.append(abs((math.pow(predicted_value-(ratios['Prediction'].values)[i],1))))\n        if i+1>=window_size:\n            predicted_value=int(sum((ratios['Prediction'].values)[(i+1)-window_size:(i+1)])/window_size)\n        else:\n            predicted_value=int(sum((ratios['Prediction'].values)[0:(i+1)])/(i+1))\n            \n    ratios['MA_P_Predicted'] = predicted_values\n    ratios['MA_P_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"NsdCFFs_pYVI","execution":{"iopub.status.busy":"2021-12-01T08:25:25.267322Z","iopub.execute_input":"2021-12-01T08:25:25.267586Z","iopub.status.idle":"2021-12-01T08:25:25.277419Z","shell.execute_reply.started":"2021-12-01T08:25:25.267558Z","shell.execute_reply":"2021-12-01T08:25:25.276571Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 1 is optimal for getting the best results using Moving Averages using previous 2016 values therefore we get $\\begin{align}P_{t} = P_{t-1} \\end{align}$","metadata":{"id":"JXXPFJ4zpYVM"}},{"cell_type":"markdown","source":"### Weighted Moving Averages\nThe Moving Avergaes Model used gave equal importance to all the values in the window used, but we know intuitively that the future is more likely to be similar to the latest values and less similar to the older values. Weighted Averages converts this analogy into a mathematical relationship giving the highest weight while computing the averages to the latest previous value and decreasing weights to the subsequent older ones<br>","metadata":{"id":"YxUx4evtpYVM"}},{"cell_type":"markdown","source":"Weighted Moving Averages using Ratio Values - $\\begin{align}R_{t} = ( N*R_{t-1} + (N-1)*R_{t-2} + (N-2)*R_{t-3} .... 1*R_{t-n} )/(N*(N+1)/2) \\end{align}$","metadata":{"id":"FM42drlhpYVM"}},{"cell_type":"code","source":"def WA_R_Predictions(ratios,month):\n    predicted_ratio=(ratios['Ratios'].values)[0]\n    alpha=0.5\n    error=[]\n    predicted_values=[]\n    window_size=5\n    predicted_ratio_values=[]\n    for i in range(0,4464*40):\n        if i%4464==0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(int(((ratios['Given'].values)[i])*predicted_ratio))\n        error.append(abs((math.pow(int(((ratios['Given'].values)[i])*predicted_ratio)-(ratios['Prediction'].values)[i],1))))\n        if i+1>=window_size:\n            sum_values=0\n            sum_of_coeff=0\n            for j in range(window_size,0,-1):\n                sum_values += j*(ratios['Ratios'].values)[i-window_size+j]\n                sum_of_coeff+=j\n            predicted_ratio=sum_values/sum_of_coeff\n        else:\n            sum_values=0\n            sum_of_coeff=0\n            for j in range(i+1,0,-1):\n                sum_values += j*(ratios['Ratios'].values)[j-1]\n                sum_of_coeff+=j\n            predicted_ratio=sum_values/sum_of_coeff\n            \n    ratios['WA_R_Predicted'] = predicted_values\n    ratios['WA_R_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"vXv6SxDWpYVN","execution":{"iopub.status.busy":"2021-12-01T08:25:25.278947Z","iopub.execute_input":"2021-12-01T08:25:25.279475Z","iopub.status.idle":"2021-12-01T08:25:25.294929Z","shell.execute_reply.started":"2021-12-01T08:25:25.279435Z","shell.execute_reply":"2021-12-01T08:25:25.293990Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 5 is optimal for getting the best results using Weighted Moving Averages using previous Ratio values therefore we get $\\begin{align} R_{t} = ( 5*R_{t-1} + 4*R_{t-2} + 3*R_{t-3} + 2*R_{t-4} + R_{t-5} )/15 \\end{align}$","metadata":{"id":"zfNtyYV_pYVP"}},{"cell_type":"markdown","source":"Weighted Moving Averages using Previous 2016 Values - $\\begin{align}P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2) \\end{align}$","metadata":{"id":"uFAt3OVNpYVQ"}},{"cell_type":"code","source":"def WA_P_Predictions(ratios,month):\n    predicted_value=(ratios['Prediction'].values)[0]\n    error=[]\n    predicted_values=[]\n    window_size=2\n    for i in range(0,4464*40):\n        predicted_values.append(predicted_value)\n        error.append(abs((math.pow(predicted_value-(ratios['Prediction'].values)[i],1))))\n        if i+1>=window_size:\n            sum_values=0\n            sum_of_coeff=0\n            for j in range(window_size,0,-1):\n                sum_values += j*(ratios['Prediction'].values)[i-window_size+j]\n                sum_of_coeff+=j\n            predicted_value=int(sum_values/sum_of_coeff)\n\n        else:\n            sum_values=0\n            sum_of_coeff=0\n            for j in range(i+1,0,-1):\n                sum_values += j*(ratios['Prediction'].values)[j-1]\n                sum_of_coeff+=j\n            predicted_value=int(sum_values/sum_of_coeff)\n    \n    ratios['WA_P_Predicted'] = predicted_values\n    ratios['WA_P_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"XTIinxMwpYVR","execution":{"iopub.status.busy":"2021-12-01T08:25:25.296717Z","iopub.execute_input":"2021-12-01T08:25:25.297092Z","iopub.status.idle":"2021-12-01T08:25:25.312464Z","shell.execute_reply.started":"2021-12-01T08:25:25.297059Z","shell.execute_reply":"2021-12-01T08:25:25.311573Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 2 is optimal for getting the best results using Weighted Moving Averages using previous 2016 values therefore we get $\\begin{align} P_{t} = ( 2*P_{t-1} + P_{t-2} )/3 \\end{align}$","metadata":{"id":"ZgyD6f_FpYVS"}},{"cell_type":"markdown","source":"### Exponential  Weighted Moving Averages\n https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\nThrough weighted averaged we have satisfied the analogy of giving higher weights to the latest value and decreasing weights to the subsequent ones but we still do not know which is the correct weighting scheme as there are infinetly many possibilities in which we can assign weights in a non-increasing order and tune the the hyperparameter window-size. To simplify this process we use Exponential Moving Averages which is a more logical way towards assigning weights and at the same time also using an optimal window-size.\n\nIn exponential moving averages we use a single hyperparameter alpha $\\begin{align}(\\alpha)\\end{align}$ which is a value between 0 & 1 and based on the value of the hyperparameter alpha the weights and the window sizes are configured.<br>\nFor eg. If $\\begin{align}\\alpha=0.9\\end{align}$ then the number of days on which the value of the current iteration is based is~$\\begin{align}1/(1-\\alpha)=10\\end{align}$ i.e. we consider values 10 days prior before we predict the value for the current iteration. Also the weights are assigned using $\\begin{align}2/(N+1)=0.18\\end{align}$ ,where N = number of prior values being considered, hence from this it is implied that the first or latest value is assigned a weight of 0.18 which keeps exponentially decreasing for the subsequent values.","metadata":{"id":"B3ySZxBwpYVT"}},{"cell_type":"markdown","source":"$\\begin{align}R^{'}_{t} = \\alpha*R_{t-1} + (1-\\alpha)*R^{'}_{t-1}  \\end{align}$","metadata":{"id":"pYi3_WazpYVT"}},{"cell_type":"code","source":"def EA_R1_Predictions(ratios,month):\n    predicted_ratio=(ratios['Ratios'].values)[0]\n    alpha=0.6\n    error=[]\n    predicted_values=[]\n    predicted_ratio_values=[]\n    for i in range(0,4464*40):\n        if i%4464==0:\n            predicted_ratio_values.append(0)\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_ratio_values.append(predicted_ratio)\n        predicted_values.append(int(((ratios['Given'].values)[i])*predicted_ratio))\n        error.append(abs((math.pow(int(((ratios['Given'].values)[i])*predicted_ratio)-(ratios['Prediction'].values)[i],1))))\n        predicted_ratio = (alpha*predicted_ratio) + (1-alpha)*((ratios['Ratios'].values)[i])\n    \n    ratios['EA_R1_Predicted'] = predicted_values\n    ratios['EA_R1_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"WxEbvtmNpYVU","execution":{"iopub.status.busy":"2021-12-01T08:25:25.314126Z","iopub.execute_input":"2021-12-01T08:25:25.314443Z","iopub.status.idle":"2021-12-01T08:25:25.329909Z","shell.execute_reply.started":"2021-12-01T08:25:25.314359Z","shell.execute_reply":"2021-12-01T08:25:25.328849Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"$\\begin{align}P^{'}_{t} = \\alpha*P_{t-1} + (1-\\alpha)*P^{'}_{t-1}  \\end{align}$","metadata":{"id":"9Gy0JcqypYVV"}},{"cell_type":"code","source":"def EA_P1_Predictions(ratios,month):\n    predicted_value= (ratios['Prediction'].values)[0]\n    alpha=0.3\n    error=[]\n    predicted_values=[]\n    for i in range(0,4464*40):\n        if i%4464==0:\n            predicted_values.append(0)\n            error.append(0)\n            continue\n        predicted_values.append(predicted_value)\n        error.append(abs((math.pow(predicted_value-(ratios['Prediction'].values)[i],1))))\n        predicted_value =int((alpha*predicted_value) + (1-alpha)*((ratios['Prediction'].values)[i]))\n    \n    ratios['EA_P1_Predicted'] = predicted_values\n    ratios['EA_P1_Error'] = error\n    mape_err = (sum(error)/len(error))/(sum(ratios['Prediction'].values)/len(ratios['Prediction'].values))\n    mse_err = sum([e**2 for e in error])/len(error)\n    return ratios,mape_err,mse_err","metadata":{"id":"VMXz3FqwpYVV","execution":{"iopub.status.busy":"2021-12-01T08:25:25.331644Z","iopub.execute_input":"2021-12-01T08:25:25.332551Z","iopub.status.idle":"2021-12-01T08:25:25.349490Z","shell.execute_reply.started":"2021-12-01T08:25:25.332502Z","shell.execute_reply":"2021-12-01T08:25:25.348439Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"mean_err=[0]*10\nmedian_err=[0]*10\nratios_jan,mean_err[0],median_err[0]=MA_R_Predictions(ratios_jan,'jan')\nratios_jan,mean_err[1],median_err[1]=MA_P_Predictions(ratios_jan,'jan')\nratios_jan,mean_err[2],median_err[2]=WA_R_Predictions(ratios_jan,'jan')\nratios_jan,mean_err[3],median_err[3]=WA_P_Predictions(ratios_jan,'jan')\nratios_jan,mean_err[4],median_err[4]=EA_R1_Predictions(ratios_jan,'jan')\nratios_jan,mean_err[5],median_err[5]=EA_P1_Predictions(ratios_jan,'jan')","metadata":{"id":"1X11ZE7epYVW","execution":{"iopub.status.busy":"2021-12-01T08:25:25.350572Z","iopub.execute_input":"2021-12-01T08:25:25.351114Z","iopub.status.idle":"2021-12-01T08:25:46.598025Z","shell.execute_reply.started":"2021-12-01T08:25:25.351075Z","shell.execute_reply":"2021-12-01T08:25:46.596982Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## Comparison between baseline models\nWe have chosen our error metric for comparison between models as <b>MAPE (Mean Absolute Percentage Error)</b> so that we can know that on an average how good is our model with predictions and <b>MSE (Mean Squared Error)</b> is also used so that we have a clearer understanding as to how well our forecasting model performs with outliers so that we make sure that there is not much of a error margin between our prediction and the actual value","metadata":{"id":"F1DYm5pFpYVX"}},{"cell_type":"code","source":"print (\"Error Metric Matrix (Forecasting Methods) - MAPE & MSE\")\nprint (\"--------------------------------------------------------------------------------------------------------\")\nprint (\"Moving Averages (Ratios) -                             MAPE: \",mean_err[0],\"      MSE: \",median_err[0])\nprint (\"Moving Averages (2016 Values) -                        MAPE: \",mean_err[1],\"       MSE: \",median_err[1])\nprint (\"--------------------------------------------------------------------------------------------------------\")\nprint (\"Weighted Moving Averages (Ratios) -                    MAPE: \",mean_err[2],\"      MSE: \",median_err[2])\nprint (\"Weighted Moving Averages (2016 Values) -               MAPE: \",mean_err[3],\"      MSE: \",median_err[3])\nprint (\"--------------------------------------------------------------------------------------------------------\")\nprint (\"Exponential Moving Averages (Ratios) -              MAPE: \",mean_err[4],\"      MSE: \",median_err[4])\nprint (\"Exponential Moving Averages (2016 Values) -         MAPE: \",mean_err[5],\"      MSE: \",median_err[5])","metadata":{"id":"Q2y5V4hppYVX","outputId":"132dc425-bd4d-4ed1-da95-8f6e4f6ee7a5","execution":{"iopub.status.busy":"2021-12-01T08:25:46.599628Z","iopub.execute_input":"2021-12-01T08:25:46.599889Z","iopub.status.idle":"2021-12-01T08:25:46.611947Z","shell.execute_reply.started":"2021-12-01T08:25:46.599860Z","shell.execute_reply":"2021-12-01T08:25:46.610990Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"<b>Note:</b> The above comparisons are made using Jan 2015 and Jan 2016 only","metadata":{"id":"8SqIWer2pYVa"}},{"cell_type":"markdown","source":"From the above matrix it is inferred that the best forecasting model for our prediction would be:-\n$\\begin{align}P^{'}_{t} = \\alpha*P_{t-1} + (1-\\alpha)*P^{'}_{t-1}  \\end{align}$ i.e Exponential Moving Averages using 2016 Values","metadata":{"id":"mioBTOTppYVa"}},{"cell_type":"markdown","source":"## Regression Models","metadata":{"id":"eoV1TT2qpYVa"}},{"cell_type":"markdown","source":"### Train-Test Split\nBefore we start predictions using the tree based regression models we take 3 months of 2016 pickup data and split it such that for every region we have 70% data in train and 30% in test, ordered date-wise for every region\n\n- Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n- number of 10min indices for jan 2015= 24*31*60/10 = 4464\n- number of 10min indices for jan 2016 = 24*31*60/10 = 4464\n- number of 10min indices for feb 2016 = 24*29*60/10 = 4176\n- number of 10min indices for march 2016 = 24*31*60/10 = 4464\n- regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups that are happened for three months in 2016 data\n","metadata":{"id":"eN4agQdupYVb"}},{"cell_type":"code","source":"\n# we take number of pickups that are happened in last 5 10min intravels\nnumber_of_time_stamps = 5\n\n# output varaible - it will contain number of pickups 13099 for each cluster\noutput = []\n\n\n# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n# it is list of lists\ntsne_lat = []\n\n\n# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n# it is list of lists\ntsne_lon = []\n\n# we will code each day \n# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n# it is list of lists\ntsne_weekday = []\n\n# its an numbpy array, of shape (523960, 5)\n# each row corresponds to an entry in out data\n# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n# the second row will have [f1,f2,f3,f4,f5]\n# the third row will have [f2,f3,f4,f5,f6] and so on...\ntsne_feature = []\n\n\ntsne_feature = [0]*number_of_time_stamps\nfor i in range(0,40):\n    tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099)\n    tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n    tsne_weekday.append([int(((int(k/144))%7+4)%7) for k in range(5,4464+4176+4464)])\n    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n    tsne_feature = np.vstack((tsne_feature, [regions_cum[i][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i])-number_of_time_stamps)]))\n    output.append(regions_cum[i][5:])\ntsne_feature = tsne_feature[1:]","metadata":{"id":"wiVsOBxYpYVb","scrolled":true,"execution":{"iopub.status.busy":"2021-12-01T08:25:46.613573Z","iopub.execute_input":"2021-12-01T08:25:46.614101Z","iopub.status.idle":"2021-12-01T08:25:48.446565Z","shell.execute_reply.started":"2021-12-01T08:25:46.614062Z","shell.execute_reply":"2021-12-01T08:25:48.445709Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"len(tsne_lat[0])*len(tsne_lat) == tsne_feature.shape[0] == len(tsne_weekday)*len(tsne_weekday[0]) == 40*13099 == len(output)*len(output[0])","metadata":{"id":"HMv1rC51pYVc","outputId":"11a6a133-7aba-4377-d852-93b6681852c9","execution":{"iopub.status.busy":"2021-12-01T08:25:48.447672Z","iopub.execute_input":"2021-12-01T08:25:48.447900Z","iopub.status.idle":"2021-12-01T08:25:48.453791Z","shell.execute_reply.started":"2021-12-01T08:25:48.447872Z","shell.execute_reply":"2021-12-01T08:25:48.453252Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n - Till now we computed 8 features for every data point that starts from 50th min of the day\n1. cluster center lattitude\n2. cluster center longitude\n3. day of the week \n4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n8. f_t_5: number of pickups that are happened previous t-5th 10min intravel","metadata":{}},{"cell_type":"code","source":"# from the baseline models we said the exponential weighted moving avarage gives us the best error\n# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \nalpha=0.3\n\n# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n# for each cluster it will get reset\n# for every cluster it contains 13104 values\npredicted_values=[]\n\n# it is similar like tsne_lat\n# it is list of lists\n# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\npredict_list = []\ntsne_flat_exp_avg = []\nfor r in range(0,40):\n    for i in range(0,13104):\n        if i==0:\n            predicted_value= regions_cum[r][0]\n            predicted_values.append(0)\n            continue\n        predicted_values.append(predicted_value)\n        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r][i]))\n    predict_list.append(predicted_values[5:])\n    predicted_values=[]","metadata":{"id":"ofQ8zCtgpYVe","execution":{"iopub.status.busy":"2021-12-01T08:25:48.454961Z","iopub.execute_input":"2021-12-01T08:25:48.455329Z","iopub.status.idle":"2021-12-01T08:25:49.894382Z","shell.execute_reply.started":"2021-12-01T08:25:48.455301Z","shell.execute_reply":"2021-12-01T08:25:49.893675Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# train, test split : 70% 30% split\n# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n# and split it such that for every region we have 70% data in train and 30% in test,\n# ordered date-wise for every region\nprint(\"size of train data :\", int(13099*0.7))\nprint(\"size of test data :\", int(13099*0.3))","metadata":{"id":"31AvW-MApYVf","outputId":"c340d12a-b4d2-4c98-a50d-aa34835318a8","execution":{"iopub.status.busy":"2021-12-01T08:25:49.895670Z","iopub.execute_input":"2021-12-01T08:25:49.896101Z","iopub.status.idle":"2021-12-01T08:25:49.901693Z","shell.execute_reply.started":"2021-12-01T08:25:49.896070Z","shell.execute_reply":"2021-12-01T08:25:49.900707Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Extracting first 9169 timestamp values i.e 70% of 13099 (total timestamps) for our training data\ntrain_features =  [tsne_feature[i*13099:(13099*i+9169)] for i in range(0,40)]\n\n# temp = [0]*(12955 - 9068)\ntest_features = [tsne_feature[(13099*(i))+9169:13099*(i+1)] for i in range(0,40)]","metadata":{"id":"Ty3ywNTepYVh","execution":{"iopub.status.busy":"2021-12-01T08:25:49.903077Z","iopub.execute_input":"2021-12-01T08:25:49.903345Z","iopub.status.idle":"2021-12-01T08:25:49.920315Z","shell.execute_reply.started":"2021-12-01T08:25:49.903308Z","shell.execute_reply":"2021-12-01T08:25:49.919135Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\nprint(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")","metadata":{"id":"9T0p_qhipYVi","outputId":"0175f0a5-d675-4568-ca3a-3535fc267a4b","execution":{"iopub.status.busy":"2021-12-01T08:25:49.921441Z","iopub.execute_input":"2021-12-01T08:25:49.922212Z","iopub.status.idle":"2021-12-01T08:25:49.945681Z","shell.execute_reply.started":"2021-12-01T08:25:49.922178Z","shell.execute_reply":"2021-12-01T08:25:49.944703Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Extracting first 9169 timestamp values i.e 70% of 13099 (total timestamps) for our training data\ntsne_train_flat_lat = [i[:9169] for i in tsne_lat]\ntsne_train_flat_lon = [i[:9169] for i in tsne_lon]\ntsne_train_flat_weekday = [i[:9169] for i in tsne_weekday]\ntsne_train_flat_output = [i[:9169] for i in output]\ntsne_train_flat_exp_avg = [i[:9169] for i in predict_list]","metadata":{"id":"cUrc4FPepYVk","execution":{"iopub.status.busy":"2021-12-01T08:25:49.948154Z","iopub.execute_input":"2021-12-01T08:25:49.948852Z","iopub.status.idle":"2021-12-01T08:25:49.964304Z","shell.execute_reply.started":"2021-12-01T08:25:49.948766Z","shell.execute_reply":"2021-12-01T08:25:49.963658Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Extracting the rest of the timestamp values i.e 30% of 12956 (total timestamps) for our test data\ntsne_test_flat_lat = [i[9169:] for i in tsne_lat]\ntsne_test_flat_lon = [i[9169:] for i in tsne_lon]\ntsne_test_flat_weekday = [i[9169:] for i in tsne_weekday]\ntsne_test_flat_output = [i[9169:] for i in output]\ntsne_test_flat_exp_avg = [i[9169:] for i in predict_list]","metadata":{"id":"jJ4h3S58pYVl","execution":{"iopub.status.busy":"2021-12-01T08:25:49.966147Z","iopub.execute_input":"2021-12-01T08:25:49.966536Z","iopub.status.idle":"2021-12-01T08:25:49.978028Z","shell.execute_reply.started":"2021-12-01T08:25:49.966494Z","shell.execute_reply":"2021-12-01T08:25:49.977313Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# The above contains values in the form of list of lists (i.e. list of values of each region), here I made all of them in one list\ntrain_new_features = []\nfor i in range(0,40):\n    train_new_features.extend(train_features[i])\ntest_new_features = []\nfor i in range(0,40):\n    test_new_features.extend(test_features[i])","metadata":{"id":"A3_aVVZnpYVm","execution":{"iopub.status.busy":"2021-12-01T08:25:49.979728Z","iopub.execute_input":"2021-12-01T08:25:49.980615Z","iopub.status.idle":"2021-12-01T08:25:50.068411Z","shell.execute_reply.started":"2021-12-01T08:25:49.980558Z","shell.execute_reply":"2021-12-01T08:25:50.067501Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# converting lists of lists into sinle list i.e flatten\n\ntsne_train_lat = sum(tsne_train_flat_lat, [])\ntsne_train_lon = sum(tsne_train_flat_lon, [])\ntsne_train_weekday = sum(tsne_train_flat_weekday, [])\ntsne_train_output = sum(tsne_train_flat_output, [])\ntsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])","metadata":{"id":"CacNoTj8pYVo","execution":{"iopub.status.busy":"2021-12-01T08:25:50.070162Z","iopub.execute_input":"2021-12-01T08:25:50.070495Z","iopub.status.idle":"2021-12-01T08:25:50.278028Z","shell.execute_reply.started":"2021-12-01T08:25:50.070451Z","shell.execute_reply":"2021-12-01T08:25:50.277420Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# converting lists of lists into sinle list i.e flatten\n\ntsne_test_lat = sum(tsne_test_flat_lat, [])\ntsne_test_lon = sum(tsne_test_flat_lon, [])\ntsne_test_weekday = sum(tsne_test_flat_weekday, [])\ntsne_test_output = sum(tsne_test_flat_output, [])\ntsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])","metadata":{"id":"vAHbGnkopYVs","execution":{"iopub.status.busy":"2021-12-01T08:25:50.278958Z","iopub.execute_input":"2021-12-01T08:25:50.279712Z","iopub.status.idle":"2021-12-01T08:25:50.360769Z","shell.execute_reply.started":"2021-12-01T08:25:50.279676Z","shell.execute_reply":"2021-12-01T08:25:50.359823Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Preparing the data frame for our train data\ncolumns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\ndf_train = pd.DataFrame(data=train_new_features, columns=columns) \ndf_train['lat'] = tsne_train_lat\ndf_train['lon'] = tsne_train_lon\ndf_train['weekday'] = tsne_train_weekday\ndf_train['exp_avg'] = tsne_train_exp_avg\n\nprint(df_train.shape)","metadata":{"id":"j5ND-T1apYVt","outputId":"fceeca31-3d06-489e-e31e-2c8ea157bdb9","execution":{"iopub.status.busy":"2021-12-01T08:25:50.361931Z","iopub.execute_input":"2021-12-01T08:25:50.362192Z","iopub.status.idle":"2021-12-01T08:25:51.856712Z","shell.execute_reply.started":"2021-12-01T08:25:50.362163Z","shell.execute_reply":"2021-12-01T08:25:51.855543Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Preparing the data frame for our train data\ndf_test = pd.DataFrame(data=test_new_features, columns=columns) \ndf_test['lat'] = tsne_test_lat\ndf_test['lon'] = tsne_test_lon\ndf_test['weekday'] = tsne_test_weekday\ndf_test['exp_avg'] = tsne_test_exp_avg\nprint(df_test.shape)","metadata":{"id":"aa8LZ28GpYVw","outputId":"70804aeb-965f-4a5c-cb23-9e5a97d2b29a","execution":{"iopub.status.busy":"2021-12-01T08:25:51.858071Z","iopub.execute_input":"2021-12-01T08:25:51.858335Z","iopub.status.idle":"2021-12-01T08:25:52.460449Z","shell.execute_reply.started":"2021-12-01T08:25:51.858305Z","shell.execute_reply":"2021-12-01T08:25:52.459491Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"id":"eQj1LfMnpYVx","outputId":"a369ae64-f86b-4990-ca77-9f650b553423","execution":{"iopub.status.busy":"2021-12-01T08:25:52.461685Z","iopub.execute_input":"2021-12-01T08:25:52.461923Z","iopub.status.idle":"2021-12-01T08:25:52.476515Z","shell.execute_reply.started":"2021-12-01T08:25:52.461893Z","shell.execute_reply":"2021-12-01T08:25:52.475288Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"### Using Linear Regression","metadata":{"id":"rykiaxYlpYVz"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr_reg=LinearRegression().fit(df_train, tsne_train_output)\n\ny_pred = lr_reg.predict(df_test)\nlr_test_predictions = [round(value) for value in y_pred]\ny_pred = lr_reg.predict(df_train)\nlr_train_predictions = [round(value) for value in y_pred]","metadata":{"id":"a4-FX3CdpYV0","execution":{"iopub.status.busy":"2021-12-01T08:25:52.478584Z","iopub.execute_input":"2021-12-01T08:25:52.478977Z","iopub.status.idle":"2021-12-01T08:25:53.605127Z","shell.execute_reply.started":"2021-12-01T08:25:52.478884Z","shell.execute_reply":"2021-12-01T08:25:53.604452Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"### Using Random Forest Regressor","metadata":{"id":"CbdQSLMipYV1"}},{"cell_type":"code","source":"regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\nregr1.fit(df_train, tsne_train_output)","metadata":{"id":"1yH4SyUqpYV1","outputId":"c868401f-5974-4c63-90db-be1ad4ee2207","execution":{"iopub.status.busy":"2021-12-01T08:25:53.607095Z","iopub.execute_input":"2021-12-01T08:25:53.607893Z","iopub.status.idle":"2021-12-01T08:26:04.437299Z","shell.execute_reply.started":"2021-12-01T08:25:53.607843Z","shell.execute_reply":"2021-12-01T08:26:04.436449Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Predicting on test data using our trained random forest model \n\n# the models regr1 is already hyper parameter tuned\n# the parameters that we got above are found using grid search\n\ny_pred = regr1.predict(df_test)\nrndf_test_predictions = [round(value) for value in y_pred]\ny_pred = regr1.predict(df_train)\nrndf_train_predictions = [round(value) for value in y_pred]","metadata":{"id":"CB0zPii6pYV3","execution":{"iopub.status.busy":"2021-12-01T08:26:04.438782Z","iopub.execute_input":"2021-12-01T08:26:04.439103Z","iopub.status.idle":"2021-12-01T08:26:07.430788Z","shell.execute_reply.started":"2021-12-01T08:26:04.439058Z","shell.execute_reply":"2021-12-01T08:26:07.429814Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"#feature importances based on analysis using random forest\nprint (df_train.columns)\nprint (regr1.feature_importances_)","metadata":{"id":"iAw-LW-PpYV5","outputId":"c112d47f-6cb6-4be2-f212-469777c8d17e","execution":{"iopub.status.busy":"2021-12-01T08:26:07.432218Z","iopub.execute_input":"2021-12-01T08:26:07.432489Z","iopub.status.idle":"2021-12-01T08:26:07.541486Z","shell.execute_reply.started":"2021-12-01T08:26:07.432459Z","shell.execute_reply":"2021-12-01T08:26:07.540459Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"### Using XgBoost Regressor","metadata":{"id":"4p0IhKaSpYV6"}},{"cell_type":"code","source":"x_model = xgb.XGBRegressor(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=3,\n min_child_weight=3,\n gamma=0,\n subsample=0.8,\n reg_alpha=200, reg_lambda=200,\n colsample_bytree=0.8,nthread=4)\nx_model.fit(df_train, tsne_train_output)","metadata":{"id":"nP3gp-0kpYV7","outputId":"f87b350e-7419-4706-eb0e-ff01b298572c","execution":{"iopub.status.busy":"2021-12-01T08:26:07.542922Z","iopub.execute_input":"2021-12-01T08:26:07.543224Z","iopub.status.idle":"2021-12-01T08:27:18.692921Z","shell.execute_reply.started":"2021-12-01T08:26:07.543189Z","shell.execute_reply":"2021-12-01T08:27:18.692090Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# Predicting with our trained Xg-Boost regressor\n\ny_pred = x_model.predict(df_test)\nxgb_test_predictions = [round(value) for value in y_pred]\ny_pred = x_model.predict(df_train)\nxgb_train_predictions = [round(value) for value in y_pred]","metadata":{"id":"vTuNZOc6pYV_","execution":{"iopub.status.busy":"2021-12-01T08:27:18.694314Z","iopub.execute_input":"2021-12-01T08:27:18.694790Z","iopub.status.idle":"2021-12-01T08:27:21.722027Z","shell.execute_reply.started":"2021-12-01T08:27:18.694749Z","shell.execute_reply":"2021-12-01T08:27:21.721061Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# feature importances\nx_model.get_booster().get_score(importance_type='weight')","metadata":{"id":"T3U0SLHApYWA","outputId":"bd2a8dec-2724-4047-b073-28b0fd98411d","execution":{"iopub.status.busy":"2021-12-01T08:36:38.500155Z","iopub.execute_input":"2021-12-01T08:36:38.500545Z","iopub.status.idle":"2021-12-01T08:36:38.507579Z","shell.execute_reply.started":"2021-12-01T08:36:38.500512Z","shell.execute_reply":"2021-12-01T08:36:38.506729Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"### Calculating the error metric values for various models","metadata":{"id":"HN_F4XI9pYWB"}},{"cell_type":"code","source":"train_mape=[]\ntest_mape=[]\n\ntrain_mape.append((mean_absolute_error(tsne_train_output,df_train['ft_1'].values))/(sum(tsne_train_output)/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))/(sum(tsne_train_output)/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output,rndf_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output, xgb_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\ntrain_mape.append((mean_absolute_error(tsne_train_output, lr_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\n\ntest_mape.append((mean_absolute_error(tsne_test_output, df_test['ft_1'].values))/(sum(tsne_test_output)/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, df_test['exp_avg'].values))/(sum(tsne_test_output)/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, rndf_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, xgb_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))\ntest_mape.append((mean_absolute_error(tsne_test_output, lr_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))","metadata":{"id":"wxWASeeEpYWB","execution":{"iopub.status.busy":"2021-12-01T08:36:49.343115Z","iopub.execute_input":"2021-12-01T08:36:49.343419Z","iopub.status.idle":"2021-12-01T08:36:49.986446Z","shell.execute_reply.started":"2021-12-01T08:36:49.343387Z","shell.execute_reply":"2021-12-01T08:36:49.985525Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"print (\"Error Metric Matrix (Tree Based Regression Methods) -  MAPE\")\nprint (\"--------------------------------------------------------------------------------------------------------\")\nprint (\"Baseline Model -                             Train: \",train_mape[0],\"    |  Test: \",test_mape[0])\nprint (\"Exponential Averages Forecasting -           Train: \",train_mape[1],\"    |  Test: \",test_mape[1])\nprint (\"Linear Regression -                          Train: \",train_mape[3],\"    |  Test: \",test_mape[3])\nprint (\"Random Forest Regression -                   Train: \",train_mape[2],\"    |  Test: \",test_mape[2])\nprint (\"--------------------------------------------------------------------------------------------------------\")","metadata":{"id":"HVAsA4-IpYWC","outputId":"d83ffcc4-67fa-4e08-a15b-0e8df84291b5","execution":{"iopub.status.busy":"2021-12-01T08:55:52.156248Z","iopub.execute_input":"2021-12-01T08:55:52.156703Z","iopub.status.idle":"2021-12-01T08:55:52.167781Z","shell.execute_reply.started":"2021-12-01T08:55:52.156669Z","shell.execute_reply":"2021-12-01T08:55:52.167080Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"### Error Metric Matrix","metadata":{"id":"dsWghvyVpYWE"}},{"cell_type":"code","source":"print (\"Error Metric Matrix (Tree Based Regression Methods) -  MAPE\")\nprint (\"--------------------------------------------------------------------------------------------------------\")\nprint (\"Baseline Model -                             Train: \",train_mape[0],\"   |  Test: \",test_mape[0])\nprint (\"Exponential Averages Forecasting -           Train: \",train_mape[1],\"   |  Test: \",test_mape[1])\nprint (\"Linear Regression -                          Train: \",train_mape[4],\"   |  Test: \",test_mape[4])\nprint (\"Random Forest Regression -                   Train: \",train_mape[2],\"   |  Test: \",test_mape[2])\nprint (\"XgBoost Regression -                         Train: \",train_mape[3],\"   |  Test: \",test_mape[3])\nprint (\"--------------------------------------------------------------------------------------------------------\")","metadata":{"id":"ajqLUxkjpYWE","outputId":"4ad278c1-0b12-44bf-b30b-ab58857e8796","execution":{"iopub.status.busy":"2021-12-01T08:38:20.096885Z","iopub.execute_input":"2021-12-01T08:38:20.097164Z","iopub.status.idle":"2021-12-01T08:38:20.107829Z","shell.execute_reply.started":"2021-12-01T08:38:20.097138Z","shell.execute_reply":"2021-12-01T08:38:20.107052Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:\n\n- I have built a total of 5 regression models using the pickup_densities. Some models used the time series property while the other models ignored it and was treated as a proper regression problem.\n\n- We can see that XGBoost performed based on TEST_MAPE was better than other models. Linear Regression was able to achieve similar values as XGBoost. \n\n- Here Random Forest Regression performs well on Train data and not good on test data which makes it overfit.Therefore, I choose XGBoost to avoid overfitting.\n\n- Some of the more important features used by these models are pickupdensity(P(t-1),P(t-2)...) followed by EMA using the previous pickup densities(predicted).\n\n- Using these predictions, the Yellow taxi owners can decide where to station themselves during their hours of least pickup.\n\n<p style=\"font-size:18px\"><b style= \"color:red\">Note :<p style=\"font-size:15px\"><b style= \"color:black\">I have not done GridSearch CV and RandomSearch CV here as its taking too much time and high loading on RAM. I have done CrossValidation in my local system and updated best parameters here.</b> </p>","metadata":{}}]}